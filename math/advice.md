## 吴军老师提到的如何学习计算机相关的数学。

请问吴军老师，对于想要成为一个二级甚至一级的工程师的年轻人而言，应该从哪入手提高自己的数学水平呢？像基础的线形代数和统计自不必说，像更高级的图论以及其它数学，该从哪开始学习呢？

吴军

说句实话，整个中国有没有二级工程师都难讲，10年内中国不会有一级工程师。按照当时定这个标准的人朗道的话讲，全世界一级的物理学家不过5、6人而已，他自己才是2.5级。那什么算是一级工程师呢？
比如亨利·福特、本田宗一郎或者保时捷父子，他们需要开创一个产业，前两个人是迄今为止被美国汽车协会唯二发给了终身特别成就奖的人。剩下的设计了某一款传奇汽车的，到顶不过是2级工程师而已。
在计算机领域，高德纳可以算是一级，Google云计算的发明人，以及发明Google大脑的杰夫·迪恩可能可以算是1.5级，因此全中国有没有二级的工程师，我不知道。
鉴于这种情况，我倒觉得成为三级工程师比较有希望，也就是说，在已知领域能做出全世界工业界最好的产品，比如你做人脸识别，做到全世界第一，就是三级的了，如果是一个大的领域，就是2.5级。要做到这一点，可以按照下面四个步骤走：
1. **把基础打好。离散数学学起来其实不难，比高中立体几何要容易得多，自己找些书学习，再结合公开课就可以了。具体讲，可以从数理逻辑、图论学起，集合论、组合论相对次要，代数结构就不必学了。但是理解它和应用它，需要动脑筋**。对做工程的人来讲，考试考100分是没有用的，需要天天思考怎样把手头的工作做漂亮，而不是应付事情。
2. 转变思维很重要。我们这周谈了很多内容，一个核心的思想就是告诉大家计算机的思维和人的思维不一样。就拿网络爬虫这个例子讲，我们日常生活是在一个规模很小的空间，在面对兆亿规模的世界，很多在生活中没有的问题会涌现出来。思维达不到一个高层次，就想不到这些问题，当然也就解决不了相应的问题。
3. 学了新东西一定要用，要有目的地使用。很多人请了好教练纠正网球动作，上场还按照自己习惯的动作打球，这样钱、时间和努力都白费了。
4. 一定要到一个有足够多高水平工程师的环境中去工作。不要指望周围都是五级工程师，你一个人是三级。杰夫·迪恩周围有一堆2级和2.5级的工程师，他的搭档戈瓦迈特博士和他水平相当，一同发明了云计算和Google大脑，而且也是美国工程院院士。当自己的水平达到一定程度时，就需要去找高手请教了，你差太多，人家不理你，就如同总是和高手打球打成6:0，人家就不和你玩了，但是如果你能做到和高手打成2:6，他们就带你玩了，这时，就不要再去找一些臭球篓子满足于刷人家6:0了。
当然，我知道这些步骤，我说起来容易，你实践起来要下苦功夫。但是，任何一个别人都不需要下苦功夫的事情，一定没有利润可言，所谓的苦功夫，就是门槛，拦住那些有妄念的人的门槛，让你前进的道路变得不那么拥挤。

---

很多人问我，数学不好是不是学不好计算机？其实你中学和大学数学没学好并没有太大关系，因为计算机需要的是离散数学，和几何、代数、微积分都没有什么关系。不过，如果缺乏逻辑性，是搞不了计算机的。

计算机所使用的数学，其实和大部分人高中和大学所学习的不同，它需要的是 **离散数学**，而我们部分人所接触的是连续的数学，比如高中所学的几何学、代数、大学所学的微积分等等。

离散数学包括数理逻辑、集合论、组合论（组合数学）、图论、抽象的代数结构等等。它们共同的特点是，研究的东西都不是连续变化的，这和计算机的二值逻辑非常契合，而和我们日常体会的世界不太一样。

**离散数学强调一些抽象的概念**，比如对大小的比较，判定一个目标是否属于某个集合（网页搜索的第一步其实就是判断一个网页是否属于要找的集合中的一员），两个点之间是否有连接的通路，比如通过网站A，经过几步，能否找到网站B的内容。**当然，今天离散数学其实在化学、工程学、生物学中也用得很多。学习离散数学，是为了培养计算机思维方式。**

对于理工科非计算机专业的人来讲，如果只是在工作中利用计算机作为工具，最好的办法就是学好一些计算机的工具。比如使用统计的人（包括研究经济学的、证券的、生物统计的等等）需要学好SAS工具；做工程的需要掌握Matlab或者Mathematica这两个工具；如果需要编程，至少应该读过《数字配方》（Numerical recipe）这本书，里面有几乎所有的将现实中的工程问题变成计算机程序的算法，以及它的很多源代码，看懂之后，直接使用就可以了。

对于使用计算机的人，要善于利用这些工具，最忌讳什么都自己从头学，从头做。

此外，如果能看得懂，计算机算法可以学一学。

---

请问吴军老师，想要学习人工智能，怎么学习会比较好？目前我只是一枚普通的从大学毕业的计算机系的学生 ，从事前端页面开发，你可以理解成写HTML CSS代码的，感觉人工智能是未来发展的主角，想往这方面发展。

吴军

有了计算机的基础，学习过编程和计算机算法之后，学习人工智能其实并不难。

首先需要有足够的概率论和数理统计基础，这在大学就是一门课，学了这门课，基本的人工智能和机器学习的数学基础就有了。

然后学习这本书《人工智能：一种现代方法》（Artificial Intelligence: A Modern Approach），这是目前最好的、写得最清楚的教科书。它的两位作者一位是罗素教授，担任过伯克利计算机系主任，是人工智能专家。另一位是诺威格博士，是我在Google的老板，过去主管Google整个的研究部门，是一位有丰富工程经验的学者。他们一直在不断地更新这本教科书。美国80%～90%的大学人工智能课都使用这本书。

此外，对于人工智能的不同应用，需要学习一些专业知识。比如做人脸识别，就需要学习图像处理；做机器翻译，就需要学习自然语言处理。

说起来很多大学有不少大教授，却与本科生无关。麻省理工学院不是这样，学校经常会请最好的教授来讲授大学一年级的大课。比如一年级的微积分课，麻省理工会根据新生的水平，将所有新生编入几个班，每个班几百人，全部由最好的数学教授授课。麻省理工学院的教授讲课很认真，而且大部分教授都乐于教学，愿意花时间给学生答疑，这可能与美国东部地区的教授们较少在公司兼职有关，相比之下，加州或南方各个大学的教授常常在外面给公司做顾问，甚至自己办公司，没有多少精力投入到教学中。



- 离散数学 discrete math
  - 数理逻辑 mathematical logic
  - 图论 graph theory

集合论，组合论相对次要。set theory, combinatorics

---

## From Steve Yegge

Math is a lot easier to pick up after you know how to program. In fact, if you're a halfway decent programmer, you'll find it's almost a snap.

They teach math all wrong in school. Way, WAY wrong. If you teach yourself math the right way, you'll learn faster, remember it longer, and it'll be much more valuable to you as a programmer.

**The Math You Learned (And Forgot)**

Here's the math I learned in school, as far as I can remember:

- Grade School: Numbers, Counting, Arithmetic, Pre-Algebra ("story problems")
- High School: Algebra, Geometry, Advanced Algebra, Trigonometry, Pre-Calculus (conics and limits)
- College: Differential and Integral Calculus, Differential Equations, Linear Algebra, Probability and Statistics, Discrete Math

Algebra? Sure. No question. You need that. And a basic understanding of [Cartesian geometry](https://en.wikipedia.org/wiki/Analytic_geometry), too. Those are useful, and you can learn everything you need to know in a few months, give or take. But the rest of them? I think an introduction to the basics might be useful, but spending a whole semester or year on them seems ridiculous.

And even if you're planning on being a scientist or an engineer, I've found it's much easier to learn and appreciate geometry and trig after you understand what exactly math is-where it came from, where it's going, what it's for. No need to dive right into memorizing geometric proofs and trigonometric identities. But that's exactly what high schools have you do.

So the list is no good anymore. Schools are teaching us the wrong math, and they're teaching it the wrong way. It's no wonder programmers think they don't need any math: most of the math we learned isn't helping us.

**The Math They Didn't Teach You**

The math computer scientists use regularly, in real life, has very little overlap with the list above. For one thing, most of the math you learn in grade school and high school is continuous: that is, math on the real numbers. For computer scientists, **95 percent or more** of the interesting math is discrete: i.e., **math on the integers**.

For now, though, don't let the term "computer scientist" worry you. It sounds intimidating, but math isn't the exclusive purview of computer scientists. You can learn it all by yourself as a closet hacker, and be just as good (or better) at it than they are. Your background as a programmer will help keep you focused on the practical side of things.

For programmers, **the most useful branch of discrete math is probability theory**. It's the first thing they should teach you after arithmetic, in grade school. What's probability theory, you ask? Why, it's counting. How many ways are there to make a Full House in poker? Or a Royal Flush? Whenever you think of a question that starts with "how many ways…" or "what are the odds…," it's a probability question.

I think it would be nice if every math course spent a full week just introducing you to the subject, in the most fun way possible, so you know why the heck you're learning it. Heck, that's probably true for every course.

Aside from probability and discrete math, there are a few other branches of mathematics that are potentially quite useful to programmers, and they usually don't teach them in school, unless you're a math minor. This list includes:

- **Statistics**, some of which is covered in my discrete math book, but it's really a discipline of its own. A pretty important one, too, but hopefully it needs no introduction.
- **Algebra** and **Linear Algebra** (i.e., matrices). They should teach Linear Algebra immediately after algebra. It's pretty easy, and it's amazingly useful in all sorts of domains, especially machine learning.
- Mathematical logic
- **Information Theory** and **Kolmogorov Complexity**. Weird, eh? I bet none of your high schools taught either of those. They're both pretty new. Information theory is (very roughly) about data compression, and Kolmogorov Complexity is (also roughly) about algorithmic complexity (i.e., how small you can you make it, how long will it take, how elegant can the program or data structure be, things like that). They're both fun, interesting, and useful.

There are others, of course, and some of the fields overlap. But it just goes to show: the math that you'll find useful is pretty different from the math your school thought would be useful.

What about **calculus**? Everyone teaches it, so it must be important, right?

Well, calculus is actually pretty easy. Before I learned it, it sounded like one of the hardest things in the universe, right up there with quantum mechanics. Quantum mechanics is still beyond me, but calculus is nothing. After I realized programmers can learn math quickly, I picked up my Calculus textbook and got through the entire thing in about a month, reading for an hour an evening.

Calculus is all about continuums-rates of change, areas under curves, volumes of solids. Useful stuff, but the exact details involve a lot of memorization and a lot of tedium that you don't normally need as a programmer. **It's better to know the overall concepts and techniques, and go look up the details when you need them**.

Geometry, trigonometry, differentiation, integration, conic sections, differential equations, and their multidimensional and multivariate versions-these all have important applications. It's just that **you don't need to know them right this second. So it probably wasn't a great idea to make you spend years and years doing proofs and exercises with them, was it?** If you're going to spend that much time studying math, it ought to be on topics that will remain relevant to you for life.

**The Right Way To Learn Math**

The right way to learn math is breadth-first, not depth-first. You need to survey the space, learn the names of things, figure out what's what.

The right way to learn math is to ignore the actual algorithms and proofs, for the most part, and to start by learning a little bit about all the techniques: their names, what they're useful for, approximately how they're computed, how long they've been around, (sometimes) who invented them, what their limitations are, and what they're related to. Think of it as a Liberal Arts degree in mathematics.

Why? Because the first step to applying mathematics is problem identification. If you have a problem to solve, and you have no idea where to start, it could take you a long time to figure it out. But if you know it's a differentiation problem, or a convex optimization problem, or a

There are lots and lots of mathematical techniques and entire sub-disciplines out there now. If you don't know what combinatorics is, not even the first clue, then you're not very likely to be able to recognize problems for which the solution is found in combinatorics, are you?

I think the best way to start learning math is to spend 15 to 30 minutes a day surfing in Wikipedia. It's filled with articles about thousands of little branches of mathematics. You start with pretty much any article that seems interesting (e.g., String theory, say, or the Fourier transform, or Tensors, anything that strikes your fancy.) Start reading. If there's something you don't understand, click the link and read about it. Do this recursively until you get bored or tired.

Doing this will give you amazing perspective on mathematics, after a few months. You'll start seeing patterns...

**About math notation**

However, by surveying math, trying to figure out what problems people have been trying to solve (and which of these might actually prove useful to you someday), you'll start seeing patterns in the notation, and it'll stop being so alien-looking. For instance, a summation sign (capital-sigma) or product sign (capital-pi) will look scary at first, even if you know the basics. But if you're a programmer, you'll soon realize it's just a loop: one that sums values, one that multiplies them. Integration is just a summation over a continuous section of a curve, so that won't stay scary for very long, either.

Once you're comfortable with the many branches of math, and the many different forms of notation, you're well on your way to knowing a lot of useful math. Because it won't be scary anymore, and next time you see a math problem, it'll jump right out at you. "Hey," you'll think, "I recognize that. That's a multiplication sign!

**When Are Exercises Useful?**

After a year of doing part-time hobbyist catch-up math, you're going to be able to do a lot more math in your head, even if you never touch a pencil to a paper. For instance, you'll see polynomials all the time, so eventually you'll pick up on the arithmetic of polynomials by osmosis. Same with logarithms, roots, transcendentals, and other fundamental mathematical representations that appear nearly everywhere.

I'm still getting a feel for how many exercises I want to work through by hand. I'm finding that I like to be able to follow explanations (proofs) using a kind of "plausibility test"-for instance, if I see someone dividing two polynomials, I kinda know what form the result should take, and if their result looks more or less right, then I'll take their word for it. But if I see the explanation doing something that I've never heard of, or that seems wrong or impossible, then I'll dig in some more.

That's a lot like reading programming-language source code, isn't it? You don't need to hand-simulate the entire program state as you read someone's code; if you know what approximate shape the computation will take, you can simply check that their result makes sense. E.g., if the result should be a list, and they're returning a scalar, maybe you should dig in a little more. But normally you can scan source code almost at the speed you'd read English text (sometimes just as fast), and you'll feel confident that you understand the overall shape and that you'll probably spot any truly egregious errors.

With that said, I still occasionally do math exercises. If something comes up again and again (like algebra and linear algebra), then I'll start doing some exercises to make sure I really understand it.

But I'd stress this: don't let exercises put you off the math. If an exercise (or even a particular article or chapter) is starting to bore you, move on. Jump around as much as you need to. Let your intuition guide you. You'll learn much, much faster doing it that way, and your confidence will grow almost every day.

**How Will This Help Me?**

Well, it might not-not right away. Certainly it will improve your logical reasoning ability; it's a bit like doing exercise at the gym, and your overall mental fitness will get better if you're pushing yourself a little every day.

For me, I've noticed that a few domains I've always been interested in (including artificial intelligence, machine learning, natural language processing, and pattern recognition) use a lot of math. And as I've dug in more deeply, I've found that the math they use is no more difficult than the sum total of the math I learned in high school; it's just different math, for the most part. It's not harder. And learning it is enabling me to code (or use in my own code) neural networks, genetic algorithms, bayesian classifiers, clustering algorithms, image matching, and other nifty things that will result in cool applications I can show off to my friends.

Excerpt From: Steve Yegge. “A Programmer's Rantings: On Programming-Language Religions, Code Philosophies, Google Work Culture, and Other Stuff.” iBooks.

Basics:

- [discrete math](https://en.wikipedia.org/wiki/Discrete_mathematics)
- algebra and Linear Algebra
  - a basic understanding of [Cartesian geometry](https://en.wikipedia.org/wiki/Analytic_geometry)
- probability theory

Advanced:

- Statistics
- Information theory and Kolmogorov Complexity
- calculus
  - It's better to know the overall concepts and techniques, and go look up the details when you need them

Optional:

Geometry, trigonometry, differentiation, integration, conic sections, differential equations, and their multidimensional and multivariate versions-these all have important applications. It's just that **you don't need to know them right this second. So it probably wasn't a great idea to make you spend years and years doing proofs and exercises with them, was it?**


---

https://masters.cs.uchicago.edu/page/math-needed-computer-science

**Discrete Math**

This is a branch of mathematics that is mainly concerned with the uses of sets and integers, both of which are ‘discrete’, separate objects from one another. The phrase was coined in the 1980s as a catch-all for math topics that were useful for computer science students, and **has evolved into a study on how to think about problem solving in the real world using mathematical (and therefore computational) models.**

Discrete math topics often are **more concerned with reasoning than numbers – many classes begin with logical statements, mathematical proofs and induction**, the important ability to prove something true for infinite amounts of data using two finite steps. When designing programs that will be run on potentially millions of machines around the world, this will help you feel confident that the algorithms and logic you designed will work correctly, no matter the environment it is run in.

Discrete math certainly has its numerical topics as well. The study of summations and their closed algorithmic forms will help you with algorithmic optimization through use of **big-O notation**, and working with **matrices and vectors** will give you an introduction into the logic needed to work with large sets of ordered data in their programs.

When companies such as Google and Yahoo were first developing their internet algorithms, they turned to **graph theory**, another major component of discrete math. Graphs can be used to model relationships between objects and sets of objects. They are also used to model practically any algorithmic problem, and once you have defined the problem as a graph, you can solve it through graph operations like traversal or by checking for connectivity and circularity.

**Discrete math is a broad term, but it was defined as a way to group the most important topics in math for needed for computer science**. The more exposure a student has to these topics, the better they will be able to handle the challenges of software engineering.

**If you have taken a calculus class, or perhaps even an advanced algebra class, you may have been exposed to some discrete math topics.**

**Probability**

Probability is a study of the odds that a particular event will occur. This area of math is often grouped under the discrete math title but has a strong presence outside the computer world as well. Probability plays a large role in determining how to predict the way a computer program will act.

Consider an algorithm that sorts numbers into descending order. Predicting how fast this algorithm will run depends on multiple factors, and the way the algorithm is designed could result in different speeds considering a data set’s best-case, worst-case, or average-case scenarios. Probability will help you determine the odds that the data coming into the algorithm is in a best or worst case ordering, and will guide you to your best solution.

A good software engineer questions everything when it comes to their design, and probability theory is an excellent tool in helping ask those questions. Probability is covered in high school statistics classes and there are many books and websites that explore it, often using dice rolls and decks of cards as examples.

**Boolean and Binary Algebra**

A computer’s state, at its lowest level, is a mass of what we call 1’s and 0’s, and yet computers can do so much for us in our daily lives. How do 1’s and 0’s build up into websites, word processors, and video games? A computer combines and compares these numbers through the use of Boolean algebra, a topic that can also help an engineer build effective logic in their own algorithms.

There is much value in studying the interplay of binary numbers in the computer. What should a computer do if the result of addition or multiplication cannot fit into a value of limited (often 32-bit) space? Did you know that the number 0.1 is an infinite decimal number when using a binary number system, just like 1/3 is infinite in our everyday base-10 system? What does that mean for a programmer? Understanding how the limited precision of the computer can model the infinite precision of the real world will give you a major leg up in the professional world.

This topic is often touched on in early computer science classes, though a basic book on computer systems or boolean algebra will also serve as a great resource.

**Recurrences**

A computer program often sees a core portion of its code (approximately 20%) run for most of the time (the other 80%). This 80-20 rule, as it is often referred to, will lead to certain functions getting called often. It’s possible that a software engineer may use a function to get a result, and then pass that result directly back into the function again. They may even see a function call itself! This concept is known as recursion, and the mathematical representation of them is called a recurrence.

The most common example of a recurrence is the Tower of Hanoi problem, where the user must move a pile of discs from one post to another (try it online!). The number of steps it takes to move a particular amount of discs can be found directly by applying some simple algebra to the answer for one less than this amount of discs. In the computer, you might solve this by having the computer call the same function over and over again, until it gets to a **base case**x that returns a simple solution, and then you would add from there.

Recursion is one of the first ‘hard’ problems a computer science student will run into, but understanding how mathematical recurrences work will help you comprehend this vital subject, and will ultimately be crucial in understanding a program’s flow. A discrete math book can give you exercises on working with summations and recurrences, though recursion itself is best taught through programming exercises, like in our summer session.

The MPCS teaches a prerequisite course in Math for Computer Science which covers a subset of the math that will be most useful for success in the program and for a career in technology. For a complete list of the topics covered in our Math for Computer Science class, please view the course website.
